{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started With Langchain And Open AI\n",
    "\n",
    "In this quickstart we'll see how to:\n",
    "\n",
    "- Get setup with LangChain, LangSmith and LangServe\n",
    "- Use the most basic and common components of LangChain: prompt templates, models, and output parsers.\n",
    "- Build a simple application with LangChain\n",
    "- Trace your application with LangSmith\n",
    "- Serve your application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=\"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\"\n",
    "\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "# Print the OpenAI API key\n",
    "print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x125cf9ed0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x125f1d120> root_client=<openai.OpenAI object at 0x125cfacb0> root_async_client=<openai.AsyncOpenAI object at 0x125cfb640> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input and get response form LLM\n",
    "\n",
    "result=llm.invoke(\"What is generative AI?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Generative AI refers to a category of artificial intelligence systems designed to generate new content or data that resembles the data on which they were trained. This technology uses models, often based on deep learning architectures like neural networks, to produce outputs ranging from text, images, and music to more complex data types.\\n\\nThere are several types of generative AI models, with some of the most prominent being:\\n\\n1. **Generative Adversarial Networks (GANs):** These consist of two neural networks, a generator and a discriminator, that are trained together. The generator creates new data instances, while the discriminator evaluates them. The goal is for the generator to produce data that the discriminator cannot distinguish from real data.\\n\\n2. **Variational Autoencoders (VAEs):** These models work by encoding input data into a latent space and then decoding it back to the original space, with the ability to sample from the latent space to generate new data.\\n\\n3. **Transformer-based Models:** These include models like GPT (Generative Pre-trained Transformer) which are particularly effective in generating coherent and contextually relevant text based on a given prompt. They have been extended to other domains as well, such as generating images or even music.\\n\\nGenerative AI has a wide range of applications, including:\\n\\n- **Content Creation:** Writing articles, creating artwork, composing music, and more.\\n- **Data Augmentation:** Enhancing datasets for training other machine learning models.\\n- **Simulation:** Creating realistic scenarios for virtual environments.\\n- **Design and Creativity:** Assisting in product design and creative processes by generating novel concepts.\\n\\nDespite its capabilities, generative AI also raises ethical and societal concerns, including issues of intellectual property, authenticity, and potential misuse for creating misinformation or fake content. As such, its development and deployment are subjects of active research and policy discussions.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 368, 'prompt_tokens': 13, 'total_tokens': 381, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_726d488742', 'id': 'chatcmpl-BLvgRSqlbgeDrAucL3VnCo7gMtkPx', 'finish_reason': 'stop', 'logprobs': None} id='run-902141d3-a3e6-4804-8b86-551e81a39bcb-0' usage_metadata={'input_tokens': 13, 'output_tokens': 368, 'total_tokens': 381, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are an expert AI Engineer. Provide me answers based on the questions'), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={})])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Chatprompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an expert AI Engineer. Provide me answers based on the questions\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    "\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Langsmith is a suite of tools developed by LangChain, designed to enhance and streamline the process of building, testing, and evaluating applications that use large language models (LLMs). It focuses on improving the development workflow for language model-driven applications by providing features for monitoring, debugging, and assessing performance.\\n\\nKey features of Langsmith include:\\n\\n1. **Monitoring**: It allows developers to monitor the behavior of LLMs in real-time, tracking various metrics to ensure models are performing as expected.\\n\\n2. **Evaluation**: Langsmith provides tools for evaluating the performance of language models, which can include both automated metrics and insights on model output quality.\\n\\n3. **Debugging**: It helps in identifying and diagnosing issues within language model operations, facilitating a smoother development process by highlighting errors or performance bottlenecks.\\n\\n4. **User-Friendly Interface**: The tools are designed with an intuitive interface to make it accessible for developers to use and integrate into their workflows effectively.\\n\\nLangsmith thus acts as a comprehensive toolkit to aid developers in harnessing the power of LLMs more efficiently and reliably, addressing common challenges in deployment and maintenance of these systems.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 231, 'prompt_tokens': 33, 'total_tokens': 264, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_92f14e8683', 'id': 'chatcmpl-BLvgkPFQ2m7IjOWB3Tx0mqOGB7aYP', 'finish_reason': 'stop', 'logprobs': None} id='run-77177f19-c909-4b67-9407-e4091ef16526-0' usage_metadata={'input_tokens': 33, 'output_tokens': 231, 'total_tokens': 264, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "## chain \n",
    "chain=prompt|llm\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langsmith is a toolset designed to aid developers in building advanced language model applications with enhanced control, observability, and performance tuning. Officially launched by LangChain in July 2023, Langsmith provides a platform for developers to effectively prototype, debug, and scale their applications. \n",
      "\n",
      "The core features of Langsmith include:\n",
      "\n",
      "1. **Instrumentation**: It allows seamless integration into existing LangChain applications to start tracking usage and performance metrics with just a single line of code.\n",
      "\n",
      "2. **Tracing and Debugging**: Langsmith offers capabilities to trace and debug language model calls, including detailed inspection of call inputs, outputs, and intermediate steps. This is particularly useful for understanding execution flow and diagnosing issues in the application.\n",
      "\n",
      "3. **Evaluation and Experimentation**: Users can evaluate the performance of different language model implementations and run experiments to fine-tune model outputs. Langsmith supports automated comparisons and benchmarks to aid in selecting the best configuration.\n",
      "\n",
      "4. **Deployment and Scaling**: Designed to support easy scaling, Langsmith integrates with existing infrastructure to help deploy applications efficiently while maintaining insight into performance and usage patterns.\n",
      "\n",
      "Langsmith serves as an ideal tool for developers looking to enhance the robustness and efficiency of their language model-driven applications.\n"
     ]
    }
   ],
   "source": [
    "## stroutput Parser\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt|llm|output_parser\n",
    "\n",
    "response=chain.invoke({\"input\":\"Can you tell me about Langsmith?\"})\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
